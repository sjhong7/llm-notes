---
title: 블로그1
permalink: /1/
---

# LLM을 활용한 서비스의 품질을 어떻게 확인할 수 있을까?
보통 LLM을 활용한 서비스를 만드는 회사에서는 자신이 가진 데이터와 연동하여 고유의 서비스를 만들려고 한다. 

관련 데이터를 검색해서 LLM에게 함께 참고자료로 제공해주어 원하는 답변을 할 수 있도록 도와주는 방식이 **RAG(Retrieval-Augmented Generation, 검색 증강 생성)**이다. 

RAG 기반 시스템의 품질은 크게 두 가지 방식으로 평가할 수 있다. 

1) 검색 성능 평가 
2) 답변(Generation)


1. 검색 성능 평가

RAG 시스템에서는 먼저 검색 단계에서 적절한 문서를 찾아야 한다. 검색 성능이 낮다면, 생성된 답변의 품질도 저하될 가능성이 높다. 

Precision@K, Recall@K, F1@K

검색된 상위 K개의 문서 중, 실제 관련 문서가 포함된 비율을 측정하는 방법입니다.

- Precision@K: 검색된 K개의 문서 중 실제 관련 문서가 몇 개 포함되어 있는지 확인
- Recall@K: 전체 관련 문서 중 검색된 문서가 얼마나 포함되었는지 측정
- F1@K: Precision과 Recall의 조화평균

이 세 가지는 검색한 문서의 순서는 고려하지 않은 것이고, 순서를 고려한 지표로 
MRR(Mean Reciprocal Rank), nDCG(Normalized Discounted Cumulative Gain) 등이 있다.


2. 답변(Generation) 평가

검색된 정보를 기반으로 LLM이 최종적으로 생성한 답변을 평가한다. 검색과는 다르게 LLM은 답변을 문장(자연어)으로 하기 때문에, 답변을 평가하는 것이 복잡해진다.
특정 질문에 대한 정답을 정해놓았다고 하더라도, 꼭 정답과 같은 문장을 완전히 똑같이 이야기해야만 답이라고 할 수 없기 때문이다. 

- ROUGE (Recall-Oriented Understudy for Gisting Evaluation):
  생성된 답변과 정답(Reference) 간의 겹치는 단어 수를 기반으로 평가

- BLEU (Bilingual Evaluation Understudy):
  기계 번역 성능을 평가하기 위해 개발되었지만, 생성된 답변의 품질 평가에도 사용됨

이런 평가 지표를 통해 여러가지 비교평가를 할 수 있게 된다는 것이 중요하다. 
서비스를 만들기 위해서 어떤 모델을 선택할지, 검색 결과 K개를 몇개 가지고 올지, 데이터를 어떻게 Chunking할지 등 다양한 변수들이 있다. 
다양한 조합으로 RAG 파이프라인을 비교 평가해보면서 최적의 과정을 찾을 수 있게 된다. 평가는 이런 성능 개선의 지표로 활용되어야 할 것이다. 

