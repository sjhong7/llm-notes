---
title: Papers
permalink: /papers/
---

# LLM 논문 요약 정리

이 페이지는 대형 언어 모델(LLM)과 관련된 주요 논문의 요약과 링크를 제공합니다.

---

## 1. GPT-3: Language Models are Few-Shot Learners
- **논문 링크:** [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
- **핵심 내용:**
  - OpenAI에서 개발한 1750억 개의 파라미터를 가진 대규모 언어 모델(GPT-3에 대한 논문)
  - 학습 데이터의 양이 많을 때, 모델의 파라미터 개수가 증가 → 성능 증가를 증명(모델 사이즈 대형화 시대를 열게 됨)
  - 각 Task마다 특화된 미세 조정(Fine-tuning) 없이 프롬프트로 다양한 문제를 해결할 수 있음을 보여줌 
  - 프롬프트에 예제를 같이 주는 것이 성능 향상에 

---

## 2. RLHF: Training a Helpful and Harmless Assistant with RLHF
- **논문 링크:** [https://arxiv.org/abs/2204.05862](https://arxiv.org/abs/2204.05862)
- **핵심 내용:**

---

## 3. CoT: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
- **논문 링크:** [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)
- **핵심 내용:**

---

## 기타 참고 자료
- 최신 AI 논문 in Arxiv: [Arxiv - Computation and Language](https://arxiv.org/list/cs.CL/recent)
- OpenAI 블로그: [https://openai.com/research](https://openai.com/research)

---
